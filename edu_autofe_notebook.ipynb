{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class EduAutoFE:\n",
    "    \n",
    "    def __init__(self, max_results=10, max_minutes=None):\n",
    "        self.max_results = max_results\n",
    "        self.max_minutes = max_minutes\n",
    "        self.results = []\n",
    "        \n",
    "    def _validate_input(self, X, y):\n",
    "        \n",
    "        # X needs to be a dataframe\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\n",
    "                \"Error: X must be a pandas DataFrame\\n\\n\"\n",
    "                \"Fix: X = pd.DataFrame(X)\"\n",
    "            )\n",
    "        \n",
    "        # Y needs to be a series\n",
    "        if not isinstance(y, pd.Series):\n",
    "            raise TypeError(\n",
    "                \"Error: y must be a pandas Series\\n\\n\"\n",
    "                'Fix: y = pd.Series(y) or y = df[\"target_column\"]'\n",
    "            )\n",
    "        \n",
    "        # Check for too many columns\n",
    "        if len(X.columns) > 100:\n",
    "            raise ValueError(\n",
    "                f\"Error: Too many columns ({len(X.columns)})\\n\\n\"\n",
    "                f\"This happens when your dataset has too many features\\n\"\n",
    "                f\"or one-hot encoding created too many columns.\\n\\n\"\n",
    "                f'Drop with: X = X.drop(columns=[\"col1\", \"col2\", ...])'\n",
    "            )\n",
    "        \n",
    "        # Check for ID columns\n",
    "        id_patterns = [\"id\", \"index\", \"key\", \"number\", \"code\"]\n",
    "        for col in X.columns:\n",
    "            col_lower = col.lower()\n",
    "            if any(pattern in col_lower for pattern in id_patterns):\n",
    "                if X[col].dtype in [np.int64, np.float64]:\n",
    "                    uniqueness = X[col].nunique() / len(X)\n",
    "                    if uniqueness > 0.95:\n",
    "                        raise ValueError(\n",
    "                            f\"Error: '{col}' looks like an ID column\\n\\n\"\n",
    "                            f\"Drop with: X = X.drop(columns=['{col}'])\"\n",
    "                        )\n",
    "        \n",
    "        # Check for text columns\n",
    "        cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "        if cat_cols:\n",
    "            raise ValueError(\n",
    "                f\"Error: Found text columns: {cat_cols}\\n\\n\"\n",
    "                f\"One-hot encode with: X = pd.get_dummies(X, drop_first=True, dtype=int)\\n\"\n",
    "                f'Drop with: X = X.drop(columns=[\"text_col\"])\\n'\n",
    "                f'Label encode with: X[\"col\"] = LabelEncoder().fit_transform(X[\"col\"])'\n",
    "            )\n",
    "        \n",
    "        # Check for missing values in X\n",
    "        if X.isnull().any().any():\n",
    "            cols_with_missing = X.columns[X.isnull().any()].tolist()\n",
    "            raise ValueError(\n",
    "                f\"Error: Missing values in: {cols_with_missing}\\n\\n\"\n",
    "                f\"Fill with median: X = X.fillna(X.median())\\n\"\n",
    "                f\"Fill with mean: X = X.fillna(X.mean())\\n\"\n",
    "                f'Drop with: X = X.drop(columns=[\"col_with_missing\"])'\n",
    "            )\n",
    "        \n",
    "        # Check for missing values in y\n",
    "        if y.isnull().any():\n",
    "            raise ValueError(\n",
    "                f\"Error: {y.isnull().sum()} missing values in y\\n\\n\"\n",
    "                f\"Fill with: y = y.fillna(y.median())\"\n",
    "            )\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def _detect_task_type(self, y):\n",
    "        n_unique = y.nunique()\n",
    "        \n",
    "        if n_unique == 2:\n",
    "            # Binary classification\n",
    "            print(f\"Detected: Binary Classification ({n_unique} classes)\")\n",
    "            print(f\"   Model: Logistic Regression\")\n",
    "            print(f\"   Why: Good for learning how features affect binary outcomes\\n\")\n",
    "            return \"classification\"\n",
    "        else:\n",
    "            # Regression\n",
    "            print(f\"Detected: Regression ({n_unique} unique values)\")\n",
    "            print(f\"   Model: Linear Regression\")\n",
    "            print(f\"   Why: Good for learning how features affect continuous outcomes\\n\")\n",
    "            return \"regression\"\n",
    "    \n",
    "    def _get_model_and_scoring(self):\n",
    "        if self.task_type == \"regression\":\n",
    "            return LinearRegression(), \"r2\", \"R2\"\n",
    "        else:\n",
    "            return LogisticRegression(max_iter=1000, random_state=42), \"accuracy\", \"Accuracy\"\n",
    "    \n",
    "    def _evaluate_feature(self, X_base, new_feature, y):\n",
    "        scaler = StandardScaler()\n",
    "        model, scoring, _ = self._get_model_and_scoring()\n",
    "        \n",
    "        # Concatenate new feature with existing ones\n",
    "        X_with_new = pd.concat([X_base, new_feature], axis=1)\n",
    "        \n",
    "        # Scale and run cross validation\n",
    "        X_scaled = scaler.fit_transform(X_with_new)\n",
    "        scores = cross_val_score(model, X_scaled, y, cv=5, scoring=scoring)\n",
    "        \n",
    "        return scores.mean(), scores.std()\n",
    "    \n",
    "    def _generate_candidates(self, X):\n",
    "        candidates = []\n",
    "        \n",
    "        # Single column transformations with descriptions\n",
    "        transforms = {\n",
    "            \"log\": (lambda x: np.log(x + 1), \"Log transformation of {col}. Compresses the long tail in the high part of heavy-tailed distributions and expands the low part, making data more normally distributed. \\nCommon applications: data that spreads over several orders of magnitude, such as prices, populations, incomes, number of reviews, word frequencies, and sales figures.\"),\n",
    "            \"sqrt\": (lambda x: np.sqrt(np.abs(x)), \"Square root of {col}. Used for Poisson-distributed data where variance equals the mean. Stabilizes variance so it is no longer dependent on the mean. Also used for compressing the long tail and strengthening the signal. \\nCommon applications: count data, event frequencies, patient measurements with extreme values (like weight or blood pressure), and visit counts.\"),\n",
    "            \"square\": (lambda x: x ** 2, \"Square of {col}. Polynomial transformation used to capture non-linear patterns in data, which is especially valuable for linear models that have difficulty finding these relationships on their own. Adds higher-order components to create new, more complex features. \\nCommon applications: variables with quadratic relationships, such as age effects, distance calculations, and diminishing returns patterns.\"),\n",
    "            \"cube\": (lambda x: x ** 3, \"Cube of {col}. Polynomial transformation used to capture non-linear patterns in data, which is especially valuable for linear models that have difficulty finding these relationships on their own. Adds higher-order components to create new, more complex features. \\nCommon applications: variables with strong non-linear relationships, such as accelerating growth patterns, S-shaped curves, and compound effects.\"),\n",
    "        }\n",
    "\n",
    "        # Generate single column transformations\n",
    "        for col in X.columns:\n",
    "            for name, (func, desc_template) in transforms.items():\n",
    "                try:\n",
    "                    transformed = func(X[col])\n",
    "                    \n",
    "                    # Skip if we get NaN or infinity\n",
    "                    if pd.isna(transformed).any() or np.isinf(transformed).any():\n",
    "                        continue\n",
    "                    \n",
    "                    feature_name = f\"{name}({col})\"\n",
    "                    feature_series = pd.Series(transformed, index=X.index, name=feature_name)\n",
    "                    description = desc_template.format(col=col)\n",
    "                    \n",
    "                    candidates.append((feature_name, feature_series, description))\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Pairwise operations with descriptions\n",
    "        operations = {\n",
    "            \"multiply\": (lambda a, b: a * b, \"Multiplication of {col1} and {col2}. Captures interaction effects and combined impact. \\nCommon applications: creating area features (length × width), calculating population (area × density), computing total cost (price × quantity), and modeling combined effects.\"),\n",
    "            \"divide\": (lambda a, b: a / (b + 1e-5), \"Division of {col1} by {col2}. Used to create per-unit measures by dividing one variable by another. \\nCommon applications: calculating BMI (weight ÷ height²), price efficiency (price ÷ area), density metrics (population ÷ area), and normalized rates.\"),\n",
    "        }\n",
    "        \n",
    "        # Only do pairwise on continuous columns, not binary ones\n",
    "        continuous_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
    "\n",
    "        # Generate pairwise transformations\n",
    "        for i, col1 in enumerate(continuous_cols):\n",
    "            for col2 in continuous_cols[i+1:]:\n",
    "                for op_name, (func, desc_template) in operations.items():\n",
    "                    try:\n",
    "                        transformed = func(X[col1], X[col2])\n",
    "                        \n",
    "                        # Skip invalid values\n",
    "                        if pd.isna(transformed).any() or np.isinf(transformed).any():\n",
    "                            continue\n",
    "                        \n",
    "                        feature_name = f\"{col1} {op_name} {col2}\"\n",
    "                        feature_series = pd.Series(transformed, index=X.index, name=feature_name)\n",
    "                        description = desc_template.format(col1=col1, col2=col2)\n",
    "                        \n",
    "                        candidates.append((feature_name, feature_series, description))\n",
    "                    except:\n",
    "                        continue\n",
    "        \n",
    "        return candidates, len(continuous_cols)\n",
    "\n",
    "    def _print_results(self, df, baseline_score, baseline_std):\n",
    "        metric = \"Accuracy\" if self.task_type == \"classification\" else \"R2\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"{\"RESULTS\":^60}\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nBaseline {metric}: {baseline_score:.4f} +/- {baseline_std:.4f}\")\n",
    "        \n",
    "        if len(df) > 0:\n",
    "            best = df.iloc[0][\"improvement\"]\n",
    "            print(f\"Best improvement: +{best:.4f}\")\n",
    "        \n",
    "        print(f\"\\nTop {len(df)} Transformations:\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            print(f\"\\n{i+1}. {row[\"feature\"]}\")\n",
    "            print(f\"{metric}: {row[\"score\"]:.4f} +/- {row[\"std\"]:.4f} (+{row[\"improvement\"]:.4f})\")\n",
    "            print(f\"{row[\"description\"]}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        \n",
    "        print(\"\\nWhat does this mean?\")\n",
    "        if self.task_type == \"classification\":\n",
    "            print(\"Accuracy shows what % of predictions are correct.\")\n",
    "            print(\"Higher accuracy = better model performance.\")\n",
    "            print(\"+/- std shows consistency across different data splits.\")\n",
    "        else:\n",
    "            print(\"R2 shows what % of variation in the target the model explains.\")\n",
    "            print(\"R2 = 0.8 means the model explains 80% of the pattern.\")\n",
    "            print(\"+/- std shows consistency across different data splits.\")\n",
    "        \n",
    "        print(\"\\nNext steps:\")\n",
    "        print(\"1. Try applying these transformations to your data\")\n",
    "        print(\"2. Combine multiple good transformations together\")\n",
    "        print(\"3. Test with other models (Random Forest, XGBoost, etc.)\")\n",
    "        print(\"4. Remember: these transformations help most with linear models!\")\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Validate input\n",
    "        X, y = self._validate_input(X, y)\n",
    "        \n",
    "        # Set task type\n",
    "        self.task_type = self._detect_task_type(y)\n",
    "        \n",
    "        # Calculate baseline\n",
    "        print(\"Calculating baseline performance...\")\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        model, scoring, metric = self._get_model_and_scoring()\n",
    "        scores = cross_val_score(model, X_scaled, y, cv=5, scoring=scoring)\n",
    "        baseline_score = scores.mean()\n",
    "        baseline_std = scores.std()\n",
    "        \n",
    "        # Generate all candidates\n",
    "        print(f\"\\nGenerating candidate transformations...\")\n",
    "        candidates, n_continuous = self._generate_candidates(X)\n",
    "        \n",
    "        # Randomize order so time limited runs test different candidates each time\n",
    "        random.shuffle(candidates)\n",
    "\n",
    "        # Print the amount of candidates created\n",
    "        print(f\"   Generated {len(candidates)} candidates\")\n",
    "        \n",
    "        # Print which mode is used\n",
    "        if self.max_minutes:\n",
    "            print(f\"\\nEvaluating candidates (max {self.max_minutes} minutes)...\")\n",
    "        else:\n",
    "            print(f\"\\nEvaluating candidates (exhaustive search)...\")\n",
    "        print(f\"   This may take a moment...\\n\")\n",
    "        \n",
    "        # Track time\n",
    "        start_time = time.time()\n",
    "        candidates_tested = 0\n",
    "        \n",
    "        # Evaluate each candidate\n",
    "        for feature_name, feature_series, description in candidates:\n",
    "            # Check time limit\n",
    "            if self.max_minutes:\n",
    "                elapsed_minutes = (time.time() - start_time) / 60\n",
    "                if elapsed_minutes >= self.max_minutes:\n",
    "                    print(f\"   Time limit reached ({self.max_minutes} min)\")\n",
    "                    print(f\"   Tested {candidates_tested} of {len(candidates)} candidates\\n\")\n",
    "                    break\n",
    "            \n",
    "            try:\n",
    "                # Wrap in dataframe for concat\n",
    "                new_feature_df = pd.DataFrame({feature_name: feature_series})\n",
    "                \n",
    "                score, std = self._evaluate_feature(X, new_feature_df, y)\n",
    "                improvement = score - baseline_score\n",
    "                \n",
    "                candidates_tested += 1\n",
    "                \n",
    "                # Only keep if it improves the score\n",
    "                if improvement > 0:\n",
    "                    self.results.append({\n",
    "                        \"feature\": feature_name,\n",
    "                        \"score\": score,\n",
    "                        \"std\": std,\n",
    "                        \"improvement\": improvement,\n",
    "                        \"description\": description\n",
    "                    })\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Check if there were no features that improved the performance\n",
    "        if not self.results:\n",
    "            print(\"No improvements found over baseline.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Sort results by improvement\n",
    "        self.results.sort(key=lambda x: x[\"improvement\"], reverse=True)\n",
    "        results_df = pd.DataFrame(self.results[:self.max_results])\n",
    "        \n",
    "        # Print the results\n",
    "        self._print_results(results_df, baseline_score, baseline_std)\n",
    "        \n",
    "        return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                                               Name  \\\n",
       "0              1       3                            Braund, Mr. Owen Harris   \n",
       "1              2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2              3       3                             Heikkinen, Miss. Laina   \n",
       "3              4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4              5       3                           Allen, Mr. William Henry   \n",
       "..           ...     ...                                                ...   \n",
       "886          887       2                              Montvila, Rev. Juozas   \n",
       "887          888       1                       Graham, Miss. Margaret Edith   \n",
       "888          889       3           Johnston, Miss. Catherine Helen \"Carrie\"   \n",
       "889          890       1                              Behr, Mr. Karl Howell   \n",
       "890          891       3                                Dooley, Mr. Patrick   \n",
       "\n",
       "        Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "0      male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1    female  38.0      1      0          PC 17599  71.2833   C85        C  \n",
       "2    female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3    female  35.0      1      0            113803  53.1000  C123        S  \n",
       "4      male  35.0      0      0            373450   8.0500   NaN        S  \n",
       "..      ...   ...    ...    ...               ...      ...   ...      ...  \n",
       "886    male  27.0      0      0            211536  13.0000   NaN        S  \n",
       "887  female  19.0      0      0            112053  30.0000   B42        S  \n",
       "888  female   NaN      1      2        W./C. 6607  23.4500   NaN        S  \n",
       "889    male  26.0      0      0            111369  30.0000  C148        C  \n",
       "890    male  32.0      0      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 11 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your data\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "X = df.drop(columns=[\"Survived\"])\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected: Binary Classification (2 classes)\n",
      "   Model: Logistic Regression\n",
      "   Why: Good for learning how features affect binary outcomes\n",
      "\n",
      "Calculating baseline performance...\n",
      "\n",
      "Generating candidate transformations...\n",
      "   Generated 52 candidates\n",
      "\n",
      "Evaluating candidates (max 1 minutes)...\n",
      "   This may take a moment...\n",
      "\n",
      "\n",
      "============================================================\n",
      "                          RESULTS                           \n",
      "============================================================\n",
      "\n",
      "Baseline Accuracy: 0.7857 +/- 0.0184\n",
      "Best improvement: +0.0224\n",
      "\n",
      "Top 10 Transformations:\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. log(Age)\n",
      "Accuracy: 0.8081 +/- 0.0237 (+0.0224)\n",
      "Log transformation of Age. Compresses the long tail in the high part of heavy-tailed distributions and expands the low part, making data more normally distributed. \n",
      "Common applications: data that spreads over several orders of magnitude, such as prices, populations, incomes, number of reviews, word frequencies, and sales figures.\n",
      "\n",
      "2. sqrt(Age)\n",
      "Accuracy: 0.8036 +/- 0.0210 (+0.0179)\n",
      "Square root of Age. Used for Poisson-distributed data where variance equals the mean. Stabilizes variance so it is no longer dependent on the mean. Also used for compressing the long tail and strengthening the signal. \n",
      "Common applications: count data, event frequencies, patient measurements with extreme values (like weight or blood pressure), and visit counts.\n",
      "\n",
      "3. Pclass divide Age\n",
      "Accuracy: 0.7980 +/- 0.0253 (+0.0123)\n",
      "Division of Pclass by Age. Used to create per-unit measures by dividing one variable by another. \n",
      "Common applications: calculating BMI (weight ÷ height²), price efficiency (price ÷ area), density metrics (population ÷ area), and normalized rates.\n",
      "\n",
      "4. square(Age)\n",
      "Accuracy: 0.7969 +/- 0.0211 (+0.0112)\n",
      "Square of Age. Polynomial transformation used to capture non-linear patterns in data, which is especially valuable for linear models that have difficulty finding these relationships on their own. Adds higher-order components to create new, more complex features. \n",
      "Common applications: variables with quadratic relationships, such as age effects, distance calculations, and diminishing returns patterns.\n",
      "\n",
      "5. SibSp divide Parch\n",
      "Accuracy: 0.7957 +/- 0.0180 (+0.0101)\n",
      "Division of SibSp by Parch. Used to create per-unit measures by dividing one variable by another. \n",
      "Common applications: calculating BMI (weight ÷ height²), price efficiency (price ÷ area), density metrics (population ÷ area), and normalized rates.\n",
      "\n",
      "6. Pclass multiply SibSp\n",
      "Accuracy: 0.7946 +/- 0.0218 (+0.0090)\n",
      "Multiplication of Pclass and SibSp. Captures interaction effects and combined impact. \n",
      "Common applications: creating area features (length × width), calculating population (area × density), computing total cost (price × quantity), and modeling combined effects.\n",
      "\n",
      "7. Pclass multiply Parch\n",
      "Accuracy: 0.7924 +/- 0.0199 (+0.0067)\n",
      "Multiplication of Pclass and Parch. Captures interaction effects and combined impact. \n",
      "Common applications: creating area features (length × width), calculating population (area × density), computing total cost (price × quantity), and modeling combined effects.\n",
      "\n",
      "8. cube(Age)\n",
      "Accuracy: 0.7924 +/- 0.0205 (+0.0067)\n",
      "Cube of Age. Polynomial transformation used to capture non-linear patterns in data, which is especially valuable for linear models that have difficulty finding these relationships on their own. Adds higher-order components to create new, more complex features. \n",
      "Common applications: variables with strong non-linear relationships, such as accelerating growth patterns, S-shaped curves, and compound effects.\n",
      "\n",
      "9. sqrt(SibSp)\n",
      "Accuracy: 0.7924 +/- 0.0176 (+0.0067)\n",
      "Square root of SibSp. Used for Poisson-distributed data where variance equals the mean. Stabilizes variance so it is no longer dependent on the mean. Also used for compressing the long tail and strengthening the signal. \n",
      "Common applications: count data, event frequencies, patient measurements with extreme values (like weight or blood pressure), and visit counts.\n",
      "\n",
      "10. cube(Pclass)\n",
      "Accuracy: 0.7924 +/- 0.0232 (+0.0067)\n",
      "Cube of Pclass. Polynomial transformation used to capture non-linear patterns in data, which is especially valuable for linear models that have difficulty finding these relationships on their own. Adds higher-order components to create new, more complex features. \n",
      "Common applications: variables with strong non-linear relationships, such as accelerating growth patterns, S-shaped curves, and compound effects.\n",
      "\n",
      "============================================================\n",
      "\n",
      "What does this mean?\n",
      "Accuracy shows what % of predictions are correct.\n",
      "Higher accuracy = better model performance.\n",
      "+/- std shows consistency across different data splits.\n",
      "\n",
      "Next steps:\n",
      "1. Try applying these transformations to your data\n",
      "2. Combine multiple good transformations together\n",
      "3. Test with other models (Random Forest, XGBoost, etc.)\n",
      "4. Remember: these transformations help most with linear models!\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Basic preprocessing\n",
    "\n",
    "# Drop columns\n",
    "X = X.drop(columns=[\"PassengerId\"])\n",
    "X = X.drop(columns=[\"Name\"])\n",
    "X = X.drop(columns=[\"Cabin\"])\n",
    "X = X.drop(columns=[\"Ticket\"])\n",
    "\n",
    "# Encode categorical variables\n",
    "X = pd.get_dummies(X, drop_first=True, dtype=int)\n",
    "\n",
    "# Fill missing values\n",
    "X = X.fillna(X.median())\n",
    "y = y.fillna(y.median())\n",
    "\n",
    "# Use the tool\n",
    "model = EduAutoFE(max_minutes=1)\n",
    "results = model.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
